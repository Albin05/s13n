# Overfitting vs Underfitting

## Concept
In Machine Learning, our goal is to build a model that generalizes well to new, unseen data. Two common pitfalls prevent this:

* **Underfitting (Too Simple):** The model is not complex enough to capture the underlying patterns in the data. It performs poorly on both training data and new data.
* **Overfitting (Too Complex):** The model is too complex and learns the "noise" or random fluctuations in the training data as if they were important patterns. It performs perfectly on training data but fails on new data.
* **Good Fit (Just Right):** The model captures the true signal but ignores the noise.

## Real-World Analogy: The Exam
Imagine three students preparing for a math test:
1.  **Student A (Underfitting):** Barely studies. Doesn't know the formulas. Fails the practice test and the real test.
2.  **Student B (Overfitting):** Memorizes the exact answers to the specific practice questions but doesn't understand the concepts. Gets 100% on the practice test but fails the real test when the numbers change.
3.  **Student C (Good Fit):** Understands the general concepts and formulas. Does well on both.

## Visualization
[Image of graph comparing underfitting overfitting and optimal fit]

## External Resources
* [Article: Understanding the Bias-Variance Tradeoff](https://example.com/bias-variance)
* [Video: Overfitting visual explanation](https://example.com/overfitting-video)
