# Evaluation Metrics

### **Part A: Easy Questions**

### **Q1**: What is an Evaluation Metric in Machine Learning?

- Define what an evaluation metric is in machine learning and explain its purpose.

### **Q2**: What is Accuracy?

- Explain what **accuracy** is as an evaluation metric. How is it calculated for a classification problem?

### **Q3**: What is Precision?

- Define **precision** in the context of classification. What does it tell you about a model's performance?

### **Q4**: What is Recall (Sensitivity)?

- Explain the concept of **recall** (or sensitivity) as an evaluation metric. How is it different from precision?

### **Q5**: What is the F1-Score?

- Define the **F1-score** and explain how it balances precision and recall. When would it be more appropriate to use F1-score over accuracy?

---

### **Part B: Intermediate Questions**

### **Q6**: What is the Confusion Matrix?

- Describe a **confusion matrix**. Create a confusion matrix for a binary classification example, and explain each of the terms: **True Positives (TP)**, **True Negatives (TN)**, **False Positives (FP)**, and **False Negatives (FN)**.

### **Q7**: How is AUC-ROC Used to Evaluate Models?

- Explain the **ROC curve** and **AUC (Area Under the Curve)**. How does AUC-ROC help to evaluate a modelâ€™s performance, and what does a high AUC value indicate?

### **Q8**: Difference Between Precision-Recall and ROC Curves

- When would you prefer using a **precision-recall curve** over an **ROC curve** for model evaluation? Discuss situations where precision-recall might be more informative.

---

### **Part C: Advanced Questions**

### **Q9**: Trade-off Between Precision and Recall

- In some machine learning applications, you may have to prioritize either **precision** or **recall**. Explain a real-life scenario where high precision is more important than high recall, and another scenario where high recall is more important than high precision.

### **Q10**: Evaluating Imbalanced Datasets

- Imbalanced datasets (where one class is more frequent than others) often lead to misleading accuracy results. Describe how you would evaluate a model trained on an imbalanced dataset. What evaluation metrics are more suitable in such cases and why?

---

**Submission**:

- Provide answers to theoretical questions in a clear and concise format.
- Include examples and explanations where applicable, especially for intermediate and advanced questions.