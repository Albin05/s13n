# In-Class Quiz

## Question 1
If you observe that your Loss Function is **increasing** with every iteration of Gradient Descent, what is the most likely cause?
A. The Learning Rate is too small.
B. The Learning Rate is too large.
C. The model is overfitting.
D. You have too much data.

## Question 2
What is the primary downside of setting a Learning Rate that is very, very small (e.g., $10^{-7}$)?
A. The model will overfit.
B. The model will diverge.
C. The training process will be extremely slow and computationally expensive.
D. The gradient will become zero.

## Question 3
In the context of optimization, "Overshooting" refers to:
A. Reaching the minimum faster than expected.
B. Taking a step so large that you cross the minimum and land on a higher point on the other side.
C. Using too many features.
D. Predicting a value larger than the maximum label.
