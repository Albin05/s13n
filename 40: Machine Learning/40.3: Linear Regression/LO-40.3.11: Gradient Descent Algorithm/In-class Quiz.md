# In-Class Quiz

## Question 1
In the Gradient Descent update rule $w_{new} = w_{old} - \alpha \times \text{gradient}$, what is the purpose of the minus sign ($-$) ?
A. To move the weight in the direction of the slope (uphill).
B. To move the weight in the opposite direction of the slope (downhill).
C. To ensure the weight never becomes negative.
D. To reduce the learning rate over time.

## Question 2
If your Learning Rate ($\alpha$) is set extremely high, what is the most likely outcome?
A. The model will converge very quickly.
B. The model will get stuck in a local minimum.
C. The model will overshoot the minimum and potentially diverge (cost increases to infinity).
D. The gradient becomes zero immediately.

## Question 3
Which variant of Gradient Descent updates the model's weights after seeing **every single data point** one by one?
A. Batch Gradient Descent
B. Mini-Batch Gradient Descent
C. Stochastic Gradient Descent (SGD)
D. Normal Equation
